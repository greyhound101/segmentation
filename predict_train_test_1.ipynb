{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_train_test_1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/segmentation/blob/master/predict_train_test_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "640abefd-643c-4fdb-f8b5-12594da711bd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "import glob\n",
        "import zipfile\n",
        "for path in glob.glob('/content/gdrive/MyDrive/segmentation/3ddirac (1)/*'):\n",
        "  if path.split('.')[-1]=='zip':\n",
        "    with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "      zip_ref.extractall('/content/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19uTw18ea1qM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4b4a8e-d580-4faa-b4c4-3d7f29bfd5b8"
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.7/dist-packages (2.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F"
      },
      "source": [
        "import pydicom\n",
        "mask=[]\n",
        "img=[]\n",
        "for i in range(129):\n",
        "  img.append(pydicom.dcmread('/content/PATIENT_DICOM/image_'+str(i)).pixel_array)\n",
        "  mask.append(pydicom.dcmread('/content/MASKS_DICOM/liver/image_'+str(i)).pixel_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG"
      },
      "source": [
        "import numpy as np\n",
        "img=np.stack(img)\n",
        "mask=np.stack(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bZhukFva4dF"
      },
      "source": [
        "img=img.transpose((1,2,0))\n",
        "mask=mask.transpose((1,2,0))\n",
        "img[img < -200] = -200\n",
        "img[img > 250] = 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW-43svEa5Sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e06efb-f42a-4b93-fef9-4707e607244c"
      },
      "source": [
        "mask[mask==255]=1\n",
        "np.unique(mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80849964-ebed-4422-d74a-014cadff17eb"
      },
      "source": [
        "pip install medpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: medpy in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (1.19.5)\n",
            "Requirement already satisfied: SimpleITK>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (2.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJem4-mp8otc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a369c478-b614-49d9-aa01-a02b9d0867e9"
      },
      "source": [
        "pip install tensorflow==1.13.1 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.7/dist-packages (1.13.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (54.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587a1fa4-eebf-473a-dd68-f2bac9d52986"
      },
      "source": [
        "pip install keras==2.2.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d6a81a-d56f-4c5c-fc3b-ebe4281afc6c"
      },
      "source": [
        "!git clone https://github.com/xmengli999/H-DenseUNet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'H-DenseUNet' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVk9YO8elt-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6a25fa-40ad-485e-dc07-a80f9841cb53"
      },
      "source": [
        "cd /content/H-DenseUNet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/H-DenseUNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykeXpxh_lu8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23544aa6-3554-49e9-9aef-d12c2ab9bcd8"
      },
      "source": [
        "pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting absl-py==0.1.10\n",
            "  Using cached https://files.pythonhosted.org/packages/5f/b8/3dafc45f20a817ab9f042302646bcbe6f7e26e8a760871a85637e53a35ec/absl-py-0.1.10.tar.gz\n",
            "Collecting backports-abc==0.5\n",
            "  Using cached https://files.pythonhosted.org/packages/7d/56/6f3ac1b816d0cd8994e83d0c4e55bc64567532f7dc543378bd87f81cebc7/backports_abc-0.5-py2.py3-none-any.whl\n",
            "Collecting backports.functools-lru-cache==1.5\n",
            "  Using cached https://files.pythonhosted.org/packages/03/8e/2424c0e65c4a066e28f539364deee49b6451f8fcd4f718fefa50cc3dcf48/backports.functools_lru_cache-1.5-py2.py3-none-any.whl\n",
            "Collecting backports.weakref==1.0rc1\n",
            "  Using cached https://files.pythonhosted.org/packages/6a/f7/ae34b6818b603e264f26fe7db2bd07850ce331ce2fde74b266d61f4a2d87/backports.weakref-1.0rc1-py3-none-any.whl\n",
            "Collecting bleach==1.5.0\n",
            "  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting bokeh==0.12.15\n",
            "  Using cached https://files.pythonhosted.org/packages/ad/67/82f17df7d1f4b9e81c9263c1a1dc3897c43cf5a9461872f9054517331f77/bokeh-0.12.15.tar.gz\n",
            "Collecting certifi==2018.1.18\n",
            "  Using cached https://files.pythonhosted.org/packages/fa/53/0a5562e2b96749e99a3d55d8c7df91c9e4d8c39a9da1f1a49ac9e4f4b39f/certifi-2018.1.18-py2.py3-none-any.whl\n",
            "Collecting cffi==1.11.5\n",
            "  Using cached https://files.pythonhosted.org/packages/51/7b/d1014289d0578c3522b2798b9cb87c65e5b36798bd3ae68a75fa1fe09e78/cffi-1.11.5-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.0.4)\n",
            "Collecting click==6.7\n",
            "  Using cached https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl\n",
            "Collecting cloudpickle==0.5.2\n",
            "  Using cached https://files.pythonhosted.org/packages/aa/18/514b557c4d8d4ada1f0454ad06c845454ad438fd5c5e0039ba51d6b032fe/cloudpickle-0.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (0.10.0)\n",
            "Collecting cytoolz==0.9.0.1\n",
            "  Using cached https://files.pythonhosted.org/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz\n",
            "Collecting dask==0.17.2\n",
            "  Using cached https://files.pythonhosted.org/packages/1d/f1/700c604af030d9b256a6590adf56cadd174c30c8ac6f555daf0e3023d294/dask-0.17.2-py2.py3-none-any.whl\n",
            "Collecting decorator==4.3.0\n",
            "  Using cached https://files.pythonhosted.org/packages/bc/bb/a24838832ba35baf52f32ab1a49b906b5f82fb7c76b2f6a7e35e140bac30/decorator-4.3.0-py2.py3-none-any.whl\n",
            "Collecting distributed==1.21.6\n",
            "  Using cached https://files.pythonhosted.org/packages/39/e8/7453e61bbee910aa91936743d6782a2108c28d9945f5f61cf801b485b5fa/distributed-1.21.6-py2.py3-none-any.whl\n",
            "Collecting dominate==2.3.1\n",
            "  Using cached https://files.pythonhosted.org/packages/43/b2/3b7d67dd59dab93ae08569384b254323516e8868b453eea5614a53835baf/dominate-2.3.1.tar.gz\n",
            "Collecting easydict==1.4\n",
            "  Using cached https://files.pythonhosted.org/packages/77/a1/dfe10522accfc2f6f27bee6144ac20f4852d6d177ec9dce1152b989d5228/easydict-1.4.tar.gz\n",
            "Collecting enum34==1.1.6\n",
            "  Using cached https://files.pythonhosted.org/packages/af/42/cb9355df32c69b553e72a2e28daee25d1611d2c0d9c272aa1d34204205b2/enum34-1.1.6-py3-none-any.whl\n",
            "Collecting funcsigs==1.0.2\n",
            "  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting functools32==3.2.3.post2\n",
            "  Using cached https://files.pythonhosted.org/packages/c5/60/6ac26ad05857c601308d8fb9e87fa36d0ebf889423f47c3502ef034365db/functools32-3.2.3-2.tar.gz\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW_cCm5qClpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d3c45ba-1c30-4892-b8b1-b9609d5cfe24"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0PuY2ltp9TB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "864e0749-ca14-453b-f11e-d96799f9ec18"
      },
      "source": [
        "#custom layers\n",
        "\n",
        "from keras.engine import Layer, InputSpec\n",
        "try:\n",
        "    from keras import initializations\n",
        "except ImportError:\n",
        "    from keras import initializers as initializations\n",
        "import keras.backend as K\n",
        "\n",
        "class Scale(Layer):\n",
        "    '''Custom Layer for DenseNet used for BatchNormalization.\n",
        "    \n",
        "    Learns a set of weights and biases used for scaling the input data.\n",
        "    the output consists simply in an element-wise multiplication of the input\n",
        "    and a sum of a set of constants:\n",
        "        out = in * gamma + beta,\n",
        "    where 'gamma' and 'beta' are the weights and biases larned.\n",
        "    # Arguments\n",
        "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
        "            if your input tensor has shape (samples, channels, rows, cols),\n",
        "            set axis to 1 to normalize per feature map (channels axis).\n",
        "        momentum: momentum in the computation of the\n",
        "            exponential average of the mean and standard deviation\n",
        "            of the data, for feature-wise normalization.\n",
        "        weights: Initialization weights.\n",
        "            List of 2 Numpy arrays, with shapes:\n",
        "            `[(input_shape,), (input_shape,)]`\n",
        "        beta_init: name of initialization function for shift parameter\n",
        "            (see [initializations](../initializations.md)), or alternatively,\n",
        "            Theano/TensorFlow function to use for weights initialization.\n",
        "            This parameter is only relevant if you don't pass a `weights` argument.\n",
        "        gamma_init: name of initialization function for scale parameter (see\n",
        "            [initializations](../initializations.md)), or alternatively,\n",
        "            Theano/TensorFlow function to use for weights initialization.\n",
        "            This parameter is only relevant if you don't pass a `weights` argument.\n",
        "    '''\n",
        "    def __init__(self, weights=None, axis=-1, momentum = 0.9, beta_init='zero', gamma_init='one', **kwargs):\n",
        "        self.momentum = momentum\n",
        "        self.axis = axis\n",
        "        self.beta_init = initializations.get(beta_init)\n",
        "        self.gamma_init = initializations.get(gamma_init)\n",
        "        self.initial_weights = weights\n",
        "        super(Scale, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        shape = (int(input_shape[self.axis]),)\n",
        "\n",
        "        # Tensorflow >= 1.0.0 compatibility\n",
        "        self.gamma = K.variable(self.gamma_init(shape), name='{}_gamma'.format(self.name))\n",
        "        self.beta = K.variable(self.beta_init(shape), name='{}_beta'.format(self.name))\n",
        "        #self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n",
        "        #self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n",
        "        self.trainable_weights = [self.gamma, self.beta]\n",
        "\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        input_shape = self.input_spec[0].shape\n",
        "        broadcast_shape = [1] * len(input_shape)\n",
        "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
        "\n",
        "        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n",
        "        return out\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
        "        base_config = super(Scale, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv"
      },
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "# sys.path.insert(0,'/home/xmli/livertumor_xmli/Keras-2.0.8')\n",
        "# sys.path.insert(0,'/home/xmli/livertumor_xmli/mylib')\n",
        "# sys.path.insert(0,'/research/pheng/xmli/livertumor/Keras-2.0.8')\n",
        "# sys.path.insert(0,'/research/pheng/xmli/livertumor/mylib')\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "import random\n",
        "from medpy.io import load\n",
        "import numpy as np\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, ZeroPadding2D, concatenate, add\n",
        "from keras.layers.core import Dropout, Activation\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.layers.pooling import AveragePooling2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import keras.backend as K\n",
        "import os\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "K.set_image_dim_ordering('tf')\n",
        "\n",
        "path = './result_train_denseU167_fast_new/'\n",
        "batch_size = 1\n",
        "img_deps = 512\n",
        "img_rows = 512\n",
        "img_cols = 3\n",
        "std = 37\n",
        "thread_num = 14\n",
        "txtfile = 'myTrainingDataTxt'\n",
        "mean = 48\n",
        "\n",
        "liverlist = [32,34,38,41,47,87,89,91,105,106,114,115,119]\n",
        "DataList = [\"/home/xmli/gpu7_xmli/\"]\n",
        "def load_seq_crop_data_masktumor_try(Parameter_List):\n",
        "    img = Parameter_List[0]\n",
        "    tumor = Parameter_List[1]\n",
        "    lines = Parameter_List[2]\n",
        "    numid = Parameter_List[3]\n",
        "    minindex = Parameter_List[4]\n",
        "    maxindex = Parameter_List[5]\n",
        "    #  randomly scale\n",
        "    scale = np.random.uniform(0.8,1.2)\n",
        "    deps = int(img_deps * scale)\n",
        "    rows = int(img_rows * scale)\n",
        "    cols = 3\n",
        "\n",
        "    sed = np.random.randint(1,numid)\n",
        "    cen = lines[sed-1]\n",
        "    cen = np.fromstring(cen, dtype=int, sep=' ')\n",
        "    # print (cen)\n",
        "    a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
        "    b = min(max(minindex[1] + rows/2, cen[1]), maxindex[1]- rows/2-1)\n",
        "    c = min(max(minindex[2] + cols/2, cen[2]), maxindex[2]- cols/2-1)\n",
        "    cropp_img = img[a - deps / 2:a + deps / 2, b - rows / 2:b + rows / 2,\n",
        "                c - cols / 2: c + cols / 2 + 1].copy()\n",
        "    cropp_tumor = tumor[a - deps / 2:a + deps / 2, b - rows / 2:b + rows / 2,\n",
        "                  c - cols / 2:c + cols / 2 + 1].copy()\n",
        "\n",
        "    cropp_img -= mean\n",
        "     # randomly flipping\n",
        "    flip_num = np.random.randint(0,3)\n",
        "    if flip_num == 1:\n",
        "        cropp_img = np.flipud(cropp_img)\n",
        "        cropp_tumor = np.flipud(cropp_tumor)\n",
        "    elif flip_num == 2:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "    #\n",
        "    cropp_tumor = resize(cropp_tumor, (img_deps,img_rows,img_cols), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
        "    cropp_img   = resize(cropp_img, (img_deps,img_rows,img_cols), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
        "    return cropp_img, cropp_tumor[:,:,1]\n",
        "\n",
        "def generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list):\n",
        "    while 1:\n",
        "        X = np.zeros((batch_size, img_deps, img_rows, img_cols), dtype='float32')\n",
        "        Y = np.zeros((batch_size, img_deps, img_rows, 1), dtype='int16')\n",
        "        Parameter_List = []\n",
        "        for idx in range(batch_size):\n",
        "            count = random.choice(trainidx)\n",
        "            img = img_list[count]\n",
        "            tumor = tumor_list[count]\n",
        "            minindex = minindex_list[count]\n",
        "            maxindex = maxindex_list[count]\n",
        "            num = np.random.randint(0,6)\n",
        "            if num < 3 or (count in liverlist):\n",
        "                lines = liverlines[count]\n",
        "                numid = liveridx[count]\n",
        "            else:\n",
        "                lines = tumorlines[count]\n",
        "                numid = tumoridx[count]\n",
        "            Parameter_List.append([img, tumor, lines, numid, minindex, maxindex])\n",
        "        pool = ThreadPool(thread_num)\n",
        "        result_list = pool.map(load_seq_crop_data_masktumor_try, Parameter_List)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        for idx in range(len(result_list)):\n",
        "            X[idx, :, :, :] = result_list[idx][0]\n",
        "            Y[idx, :, :, 0] = result_list[idx][1]\n",
        "        yield (X,Y)\n",
        "\n",
        "def weighted_crossentropy(y_true, y_pred):\n",
        "\n",
        "    y_pred_f = K.reshape(y_pred, (batch_size*img_deps*img_rows,3))\n",
        "    y_true_f = K.reshape(y_true, (batch_size*img_deps*img_rows,))\n",
        "\n",
        "    soft_pred_f = K.softmax(y_pred_f)\n",
        "    soft_pred_f = K.log(tf.clip_by_value(soft_pred_f, 1e-10, 1.0))\n",
        "\n",
        "    neg = K.equal(y_true_f, K.zeros_like(y_true_f))\n",
        "    neg_calculoss = tf.gather(soft_pred_f[:,0], tf.where(neg))\n",
        "\n",
        "    pos1 = K.equal(y_true_f, K.ones_like(y_true_f))\n",
        "    pos1_calculoss = tf.gather(soft_pred_f[:,1], tf.where(pos1))\n",
        "\n",
        "    pos2 = K.equal(y_true_f, 2*K.ones_like(y_true_f))\n",
        "    pos2_calculoss = tf.gather(soft_pred_f[:,2], tf.where(pos2))\n",
        "\n",
        "    loss = -K.mean(tf.concat([0.78*neg_calculoss, 0.65*pos1_calculoss, 8.57*pos2_calculoss], 0))\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def DenseUNet(nb_dense_block=4, growth_rate=48, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # Handle Dimension Ordering for different backends\n",
        "    global concat_axis\n",
        "    if K.image_dim_ordering() == 'tf':\n",
        "      concat_axis = 3\n",
        "      img_input = Input(batch_shape=(batch_size, img_deps, img_rows, 3), name='data')\n",
        "    else:\n",
        "      concat_axis = 1\n",
        "      img_input = Input(shape=(3, 512,512), name='data')\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [6,12,36,24] # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv1_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name='conv1_scale')(x)\n",
        "    x = Activation('relu', name='relu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx+2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv'+str(final_stage)+'_blk_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x)\n",
        "    x = Activation('relu', name='relu'+str(final_stage)+'_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling2D(size=(2,2))(x)\n",
        "    line0 = Conv2D(2208, (1, 1), padding=\"same\", kernel_initializer=\"normal\", name=\"line0\")(box[3])\n",
        "    up0_sum = add([line0, up0])\n",
        "    conv_up0 = Conv2D(768, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up0\")(up0_sum)\n",
        "    bn_up0 = BatchNormalization(name = \"bn_up0\")(conv_up0)\n",
        "    ac_up0 = Activation('relu', name='ac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling2D(size=(2,2))(ac_up0)\n",
        "    up1_sum = add([box[2], up1])\n",
        "    conv_up1 = Conv2D(384, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up1\")(up1_sum)\n",
        "    bn_up1 = BatchNormalization(name = \"bn_up1\")(conv_up1)\n",
        "    ac_up1 = Activation('relu', name='ac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling2D(size=(2,2))(ac_up1)\n",
        "    up2_sum = add([box[1], up2])\n",
        "    conv_up2 = Conv2D(96, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up2\")(up2_sum)\n",
        "    bn_up2 = BatchNormalization(name = \"bn_up2\")(conv_up2)\n",
        "    ac_up2 = Activation('relu', name='ac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2,2))(ac_up2)\n",
        "    up3_sum = add([box[0], up3])\n",
        "    conv_up3 = Conv2D(96, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up3\")(up3_sum)\n",
        "    bn_up3 = BatchNormalization(name = \"bn_up3\")(conv_up3)\n",
        "    ac_up3 = Activation('relu', name='ac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling2D(size=(2, 2))(ac_up3)\n",
        "    conv_up4 = Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name=\"conv_up4\")(up4)\n",
        "    conv_up4 = Dropout(rate=0.3)(conv_up4)\n",
        "    bn_up4 = BatchNormalization(name=\"bn_up4\")(conv_up4)\n",
        "    ac_up4 = Activation('relu', name='ac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv2D(3, (1,1), padding=\"same\", kernel_initializer=\"normal\", name=\"dense167classifer\")(ac_up4)\n",
        "\n",
        "    model = Model(img_input, x, name='denseu161')\n",
        "\n",
        "    if weights_path is not None:\n",
        "      model.load_weights(weights_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor \n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x1_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), name=conv_name_base+'_x1', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x2_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), name=conv_name_base+'_x2', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout \n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
        "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
        "    pool_name_base = 'pool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv2D(int(nb_filter * compression), (1, 1), name=conv_name_base, use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "\n",
        "\n",
        "\n",
        "def train_and_predict():\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Creating and compiling model...')\n",
        "    print('-'*30)\n",
        "\n",
        "    model = DenseUNet(reduction=0.5, weights_path='./result_train_dense167_fast/model/weights365.04-0.02.hdf5')\n",
        "    sgd = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss=[weighted_crossentropy])\n",
        "\n",
        "    trainidx = list(range(131))\n",
        "    img_list = []\n",
        "    tumor_list = []\n",
        "    minindex_list = []\n",
        "    maxindex_list = []\n",
        "    tumorlines = []\n",
        "    tumoridx = []\n",
        "    liveridx = []\n",
        "    liverlines = []\n",
        "    t1=time.time()\n",
        "    for idx in range(131):\n",
        "        img, img_header = load(DataList[0] + 'myTrainingData/volume-' + str(idx) + '.nii' )\n",
        "        tumor, tumor_header = load(DataList[0] + 'myTrainingData/segmentation-' + str(idx) + '.nii')\n",
        "        img_list.append(img)\n",
        "        tumor_list.append(tumor)\n",
        "\n",
        "        maxmin = np.loadtxt(DataList[0] + str(txtfile) + '/LiverBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
        "        minindex = maxmin[0:3]\n",
        "        maxindex = maxmin[3:6]\n",
        "        minindex = np.array(minindex, dtype='int')\n",
        "        maxindex = np.array(maxindex, dtype='int')\n",
        "        minindex[0] = max(minindex[0]-3, 0)\n",
        "        minindex[1] = max(minindex[1]-3, 0)\n",
        "        minindex[2] = max(minindex[2]-3, 0)\n",
        "        maxindex[0] = min(img.shape[0], maxindex[0]+3)\n",
        "        maxindex[1] = min(img.shape[1], maxindex[1]+3)\n",
        "        maxindex[2] = min(img.shape[2], maxindex[2]+3)\n",
        "        minindex_list.append(minindex)\n",
        "        maxindex_list.append(maxindex)\n",
        "\n",
        "        f1 = open(DataList[0] + str(txtfile) + '/TumorPixels/tumor_' + str(idx) + '.txt','r')\n",
        "        tumorline = f1.readlines()\n",
        "        tumorlines.append(tumorline)\n",
        "        tumoridx.append(len(tumorline))\n",
        "        f1.close()\n",
        "\n",
        "        f2 = open(DataList[0] + str(txtfile) + '/LiverPixels/liver_' + str(idx) + '.txt','r')\n",
        "        liverline = f2.readlines()\n",
        "        liverlines.append(liverline)\n",
        "        liveridx.append(len(liverline))\n",
        "        f2.close()\n",
        "    t2=time.time()\n",
        "    print (t2-t1)\n",
        "\n",
        "\n",
        "    # print (model.summary())\n",
        "\n",
        "    if not os.path.exists(path + \"model\"):\n",
        "        os.mkdir(path + 'model')\n",
        "        os.mkdir(path + 'history')\n",
        "    else:\n",
        "        if os.path.exists(path + \"history/lossbatch.txt\"):\n",
        "            os.remove(path + 'history/lossbatch.txt')\n",
        "        if os.path.exists(path + \"history/lossepoch.txt\"):\n",
        "            os.remove(path + 'history/lossepoch.txt')\n",
        "    model_checkpoint = ModelCheckpoint(path + 'model/weights.{epoch:02d}-{loss:.2f}.hdf5', monitor='loss', verbose = 1,\n",
        "                                       save_best_only=False,save_weights_only=False,mode = 'min', period = 2)\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Fitting model......')\n",
        "    print('-'*30)\n",
        "\n",
        "    steps = 27386/batch_size\n",
        "    model.fit_generator(generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list),steps_per_epoch=steps,\n",
        "                        epochs= 6000, verbose = 1, callbacks = [model_checkpoint], max_queue_size=10, workers=3, use_multiprocessing=True)\n",
        "\n",
        "    print ('Finised Training .......')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnurKIQWPTme"
      },
      "source": [
        "#loss\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def weighted_crossentropy(y_true, y_pred):\n",
        "    y_pred = y_pred[:,:,:,1:7,:]\n",
        "    y_true = y_true[:,:,:,1:7,:]\n",
        "    y_pred_f = K.reshape(y_pred, (-1,3))\n",
        "    y_true_f = K.reshape(y_true, (-1,))\n",
        "\n",
        "    soft_pred_f = K.softmax(y_pred_f)\n",
        "    soft_pred_f = K.log(tf.clip_by_value(soft_pred_f, 1e-10, 1.0))\n",
        "\n",
        "    neg = K.equal(y_true_f, K.zeros_like(y_true_f))\n",
        "    neg_calculoss = tf.gather(soft_pred_f[:,0], tf.where(neg))\n",
        "\n",
        "    pos1 = K.equal(y_true_f, K.ones_like(y_true_f))\n",
        "    pos1_calculoss = tf.gather(soft_pred_f[:,1], tf.where(pos1))\n",
        "\n",
        "    pos2 = K.equal(y_true_f, 2*K.ones_like(y_true_f))\n",
        "    pos2_calculoss = tf.gather(soft_pred_f[:,2], tf.where(pos2))\n",
        "\n",
        "    loss = -K.mean(tf.concat([0.78*neg_calculoss, 0.65*pos1_calculoss, 8.57*pos2_calculoss], 0))\n",
        "\n",
        "    return loss\n",
        "\n",
        "def weighted_crossentropy_2ddense(y_true, y_pred):\n",
        "\n",
        "    y_pred_f = K.reshape(y_pred, (-1,3))\n",
        "    y_true_f = K.reshape(y_true, (-1,))\n",
        "\n",
        "    soft_pred_f = K.softmax(y_pred_f)\n",
        "    soft_pred_f = K.log(tf.clip_by_value(soft_pred_f, 1e-10, 1.0))\n",
        "\n",
        "    neg = K.equal(y_true_f, K.zeros_like(y_true_f))\n",
        "    neg_calculoss = tf.gather(soft_pred_f[:,0], tf.where(neg))\n",
        "\n",
        "    pos1 = K.equal(y_true_f, K.ones_like(y_true_f))\n",
        "    pos1_calculoss = tf.gather(soft_pred_f[:,1], tf.where(pos1))\n",
        "\n",
        "    pos2 = K.equal(y_true_f, 2*K.ones_like(y_true_f))\n",
        "    pos2_calculoss = tf.gather(soft_pred_f[:,2], tf.where(pos2))\n",
        "\n",
        "    loss = -K.mean(tf.concat([0.78*neg_calculoss, 0.65*pos1_calculoss, 8.57*pos2_calculoss], 0))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79arC5XHPTjM"
      },
      "source": [
        "#changed xrange to range\n",
        "\n",
        "#train2ddense\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "#causing problems\n",
        "# import sys\n",
        "# sys.path.insert(0,'Keras-2.0.8')\n",
        "\n",
        "\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "import random\n",
        "from medpy.io import load\n",
        "import numpy as np\n",
        "import argparse\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras.backend as K\n",
        "import os\n",
        "\n",
        "\n",
        "# Make parallel\n",
        "# from keras.utils2.multi_gpu import make_parallel\n",
        "\n",
        "\n",
        "from skimage.transform import resize\n",
        "K.set_image_dim_ordering('tf')\n",
        "\n",
        "#  global parameters\n",
        "# parser = argparse.ArgumentParser(description='Keras 2d denseunet Training')\n",
        "# #  data folder\n",
        "# parser.add_argument('-data', type=str, default='data/', help='test images')\n",
        "# parser.add_argument('-save_path', type=str, default='Experiments/')\n",
        "# #  other paras\n",
        "# parser.add_argument('-b', type=int, default=1)\n",
        "# parser.add_argument('-input_size', type=int, default=224)\n",
        "# parser.add_argument('-model_weight', type=str, default='./model/densenet161_weights_tf.h5')\n",
        "# parser.add_argument('-input_cols', type=int, default=3)\n",
        "\n",
        "# #  data augment\n",
        "# parser.add_argument('-mean', type=int, default=48)\n",
        "# parser.add_argument('-thread_num', type=int, default=14)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "MEAN = 48\n",
        "thread_num = 14\n",
        "\n",
        "liverlist = [32,34,38,41,47,87,89,91,105,106,114,115,119]\n",
        "def load_seq_crop_data_masktumor_try(Parameter_List):\n",
        "    img = Parameter_List[0]\n",
        "    tumor = Parameter_List[1]\n",
        "    lines = Parameter_List[2]\n",
        "    numid = Parameter_List[3]\n",
        "    minindex = Parameter_List[4]\n",
        "    maxindex = Parameter_List[5]\n",
        "    #  randomly scale\n",
        "    scale = np.random.uniform(0.8,1.2)\n",
        "    deps = int(512 * scale)\n",
        "    rows = int(512 * scale)\n",
        "    cols = 3\n",
        "\n",
        "    sed = np.random.randint(1,numid)\n",
        "    cen = lines[sed-1]\n",
        "    cen = np.fromstring(cen, dtype=int, sep=' ')\n",
        "\n",
        "    a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
        "    b = min(max(minindex[1] + rows/2, cen[1]), maxindex[1]- rows/2-1)\n",
        "    c = min(max(minindex[2] + cols/2, cen[2]), maxindex[2]- cols/2-1)\n",
        "    \n",
        "    \n",
        "    # a1=min(img.shape[0]-int(a - deps / 2),int(a + (deps / 2)))\n",
        "    # a2=max(img.shape[0]-int(a - deps / 2),int(a + (deps / 2)))\n",
        "    # b1=min(img.shape[1]-int(b - (rows / 2)),int(b + (rows / 2)))\n",
        "    # b2=max(img.shape[1]-int(b - (rows / 2)),int(b + (rows / 2)))\n",
        "    # c1=min(img.shape[2]-int(c - (cols / 2)),int(c + (cols / 2)))\n",
        "    # c2=max(img.shape[2]-int(c - (cols / 2)),int(c + (cols / 2)))\n",
        "    \n",
        "    \n",
        "    # cropp_img = img[int(a - (deps / 2)):int(a + (deps / 2)), int(b - (rows / 2)):int(b + (rows / 2)),\n",
        "    #             int(c - (cols / 2)): int(c + (cols / 2) + 1)].copy()\n",
        "    # cropp_tumor = tumor[int(a -( deps / 2)):int(a + (deps / 2)), int(b - (rows / 2)):int(b + (rows / 2)),\n",
        "    #               int(c - (cols / 2)):int(c + (cols / 2) + 1)].copy()\n",
        "    \n",
        "    if a < deps / 2:\n",
        "      a1=int((img.shape[0]/2)-(deps/2))\n",
        "      a2=int((img.shape[0]/2)+(deps/2))\n",
        "    else:\n",
        "      a1=int(a-deps/2)\n",
        "      a2=int(a+deps/2)\n",
        "    if b < rows / 2:\n",
        "      b1=int((img.shape[0]/2)-(rows/2))\n",
        "      b2=int((img.shape[0]/2)+(rows/2))\n",
        "    else:\n",
        "      b1=int(b-rows/2)\n",
        "      b2=int(b+rows/2)\n",
        "    if c < cols / 2:\n",
        "      c1=int((img.shape[0]/2)-(cols/2))\n",
        "      c2=int((img.shape[0]/2)+(cols/2))\n",
        "    else:\n",
        "      c1=int(c-cols/2)\n",
        "      c2=int(c+cols/2)\n",
        "\n",
        "    cropp_img = img[a1:a2, b1:b2,c1: c2+1].copy()\n",
        "\n",
        "    cropp_tumor = tumor[a1:a2, b1:b2,c1: c2+1].copy()\n",
        "\n",
        "    cropp_img -= MEAN\n",
        "     # randomly flipping\n",
        "    flip_num = np.random.randint(0, 8)\n",
        "    if flip_num == 1:\n",
        "        cropp_img = np.flipud(cropp_img)\n",
        "        cropp_tumor = np.flipud(cropp_tumor)\n",
        "    elif flip_num == 2:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "    elif flip_num == 3:\n",
        "        cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=1, axes=(1, 0))\n",
        "    elif flip_num == 4:\n",
        "        cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=3, axes=(1, 0))\n",
        "    elif flip_num == 5:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "        cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=1, axes=(1, 0))\n",
        "    elif flip_num == 6:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "        cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=3, axes=(1, 0))\n",
        "    elif flip_num == 7:\n",
        "        cropp_img = np.flipud(cropp_img)\n",
        "        cropp_tumor = np.flipud(cropp_tumor)\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "\n",
        "    cropp_tumor = resize(cropp_tumor, (512,512,3), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
        "    cropp_img   = resize(cropp_img, (512,512,3), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
        "    return cropp_img, cropp_tumor[:,:,1]\n",
        "\n",
        "def generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list):\n",
        "    while 1:\n",
        "        X = np.zeros((1,512,512,3), dtype='float32')\n",
        "        Y = np.zeros((1,512,512, 1), dtype='int16')\n",
        "        Parameter_List = []\n",
        "        for idx in range(batch_size):\n",
        "            count = random.choice(trainidx)\n",
        "            img = img_list[count]\n",
        "            tumor = tumor_list[count]\n",
        "            minindex = minindex_list[count]\n",
        "            maxindex = maxindex_list[count]\n",
        "            num = np.random.randint(0,6)\n",
        "            if num < 3 or (count in liverlist):\n",
        "                lines = liverlines[count]\n",
        "                numid = liveridx[count]\n",
        "            else:\n",
        "                lines = tumorlines[count]\n",
        "                numid = tumoridx[count]\n",
        "            Parameter_List.append([img, tumor, lines, numid, minindex, maxindex])\n",
        "        pool = ThreadPool(thread_num)\n",
        "        result_list = pool.map(load_seq_crop_data_masktumor_try, Parameter_List)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        for idx in range(len(result_list)):\n",
        "            X[idx, :, :, :] = result_list[idx][0]\n",
        "            Y[idx, :, :, 0] = result_list[idx][1]\n",
        "        yield (X,Y)\n",
        "\n",
        "\n",
        "def load_fast_files():\n",
        "\n",
        "    trainidx = list(range(5))\n",
        "    img_list = []\n",
        "    tumor_list = []\n",
        "    minindex_list = []\n",
        "    maxindex_list = []\n",
        "    tumorlines = []\n",
        "    tumoridx = []\n",
        "    liveridx = []\n",
        "    liverlines = []\n",
        "    for idx in range(5):\n",
        "        img, img_header = load('/content/data'+ '/myTrainingData/volume-' + str(idx) + '.nii')\n",
        "        tumor, tumor_header = load('/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/segmentation-' + str(idx) + '.nii')\n",
        "        img_list.append(img)\n",
        "        tumor_list.append(tumor)\n",
        "\n",
        "        maxmin = np.loadtxt('/content/data' + '/myTrainingDataTxt/LiverBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
        "        minindex = maxmin[0:3]\n",
        "        maxindex = maxmin[3:6]\n",
        "        minindex = np.array(minindex, dtype='int')\n",
        "        maxindex = np.array(maxindex, dtype='int')\n",
        "        minindex[0] = max(minindex[0] - 3, 0)\n",
        "        minindex[1] = max(minindex[1] - 3, 0)\n",
        "        minindex[2] = max(minindex[2] - 3, 0)\n",
        "        maxindex[0] = min(img.shape[0], maxindex[0] + 3)\n",
        "        maxindex[1] = min(img.shape[1], maxindex[1] + 3)\n",
        "        maxindex[2] = min(img.shape[2], maxindex[2] + 3)\n",
        "        minindex_list.append(minindex)\n",
        "        maxindex_list.append(maxindex)\n",
        "        f1 = open('/content/data' + '/myTrainingDataTxt/TumorPixels/tumor_' + str(idx) + '.txt', 'r')\n",
        "        tumorline = f1.readlines()\n",
        "        tumorlines.append(tumorline)\n",
        "        tumoridx.append(len(tumorline))\n",
        "        f1.close()\n",
        "        f2 = open('/content/data'+ '/myTrainingDataTxt/LiverPixels/liver_' + str(idx) + '.txt', 'r')\n",
        "        liverline = f2.readlines()\n",
        "        liverlines.append(liverline)\n",
        "        liveridx.append(len(liverline))\n",
        "        f2.close()\n",
        "\n",
        "    return trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list\n",
        "\n",
        "def train_and_predict():\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Creating and compiling model...')\n",
        "    print('-'*30)\n",
        "\n",
        "    model = DenseUNet(reduction=0.5)\n",
        "    model.load_weights('/content/gdrive/MyDrive/segmentation/model_best.hdf5', by_name=True)\n",
        "    \n",
        "    \n",
        "    # model = make_parallel(model, args.b / 10, mini_batch=10)\n",
        "    \n",
        "    \n",
        "    sgd = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss=[weighted_crossentropy_2ddense])\n",
        "\n",
        "    trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list = load_fast_files()\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Fitting model......')\n",
        "    print('-'*30)\n",
        "    save_path='Experiments/'\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "\n",
        "    if not os.path.exists(save_path + \"/model\"):\n",
        "        os.mkdir(save_path + '/model')\n",
        "        os.mkdir(save_path + '/history')\n",
        "    else:\n",
        "        if os.path.exists(save_path+ \"/history/lossbatch.txt\"):\n",
        "            os.remove(save_path + '/history/lossbatch.txt')\n",
        "        if os.path.exists(save_path + \"/history/lossepoch.txt\"):\n",
        "            os.remove(save_path + '/history/lossepoch.txt')\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(save_path + '/model/weights.{epoch:02d}-{loss:.2f}.hdf5', monitor='loss', verbose = 1,\n",
        "                                       save_best_only=False,save_weights_only=False,mode = 'min', period = 1)\n",
        "\n",
        "\n",
        "    steps = 5\n",
        "    model.fit_generator(generate_arrays_from_file(1, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx,\n",
        "                                                  liveridx, minindex_list, maxindex_list),steps_per_epoch=steps,\n",
        "                                                    epochs= 5, verbose = 1, callbacks = [model_checkpoint], max_queue_size=10,\n",
        "                                                    workers=3, use_multiprocessing=True)\n",
        "\n",
        "    print ('Finised Training .......')\n",
        "# if __name__ == '__main__':\n",
        "#     train_and_predict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sS5g9uyXuNQ"
      },
      "source": [
        "#lib funcs\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from skimage import measure\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_tumor_inwindow(model, imgs_test, num, mini, maxi):\n",
        "\n",
        "    batch = 1\n",
        "    img_deps = 512\n",
        "    img_rows = 512\n",
        "    img_cols = 8\n",
        "\n",
        "\n",
        "    window_cols = int(img_cols/4)\n",
        "    count = 0\n",
        "    box_test = np.zeros((batch,img_deps,img_rows,img_cols, 1), dtype=\"float32\")\n",
        "\n",
        "    x = imgs_test.shape[0]\n",
        "    y = imgs_test.shape[1]\n",
        "    z = imgs_test.shape[2]\n",
        "    right_cols = int(min(z,maxi[2]+10)-img_cols)\n",
        "    left_cols  =int( max(0,min(mini[2]-5, right_cols)))\n",
        "    score = np.zeros((x, y, z, num), dtype= 'float32')\n",
        "    score_num = np.zeros((x, y, z, num), dtype= 'int16')\n",
        "\n",
        "    for cols in tqdm(range(left_cols,right_cols+window_cols,window_cols)):\n",
        "        # print ('and', z-img_cols,z)\n",
        "        if cols > z - img_cols:\n",
        "            patch_test = imgs_test[0:img_deps, 0:img_rows, z-img_cols:z]\n",
        "            box_test[count, :, :, :, 0] = patch_test\n",
        "            # print ('final', img_cols-window_cols, img_cols)\n",
        "            patch_test_mask = model.predict(box_test, batch_size=batch, verbose=0)\n",
        "            patch_test_mask = K.softmax(patch_test_mask)\n",
        "            patch_test_mask = K.eval(patch_test_mask)\n",
        "            patch_test_mask = patch_test_mask[:,:,:,1:-1,:]\n",
        "\n",
        "            for i in range(batch):\n",
        "                score[0:img_deps, 0:img_rows,  z-img_cols+1:z-1, :] += patch_test_mask[i]\n",
        "                score_num[0:img_deps, 0:img_rows,  z-img_cols+1:z-1, :] += 1\n",
        "        else:\n",
        "            patch_test = imgs_test[0:img_deps, 0:img_rows, cols:cols + img_cols]\n",
        "            box_test[count, :, :, :, 0] = patch_test\n",
        "            patch_test_mask = model.predict(box_test, batch_size=batch, verbose=0)\n",
        "            patch_test_mask = K.softmax(patch_test_mask)\n",
        "            patch_test_mask = K.eval(patch_test_mask)\n",
        "            patch_test_mask = patch_test_mask[:,:,:,1:-1,:]\n",
        "            for i in range(batch):\n",
        "                score[0:img_deps, 0:img_rows, cols+1:cols+img_cols-1, :] += patch_test_mask[i]\n",
        "                score_num[0:img_deps, 0:img_rows, cols+1:cols+img_cols-1, :] += 1\n",
        "    score = score/(score_num+1e-4)\n",
        "    score1 = score[:,:,:,num-2]\n",
        "    score2 = score[:,:,:,num-1]\n",
        "    return score1, score2\n",
        "\n",
        "\n",
        "def predict_window_mulgpu(model,batch, imgs_test, img_deps, img_rows, img_cols, multiloss):\n",
        "\n",
        "    window_deps = (img_deps/3)*2\n",
        "    window_rows = (img_rows/3)*2\n",
        "    window_cols = (img_cols/3)*2\n",
        "\n",
        "    current_test = imgs_test\n",
        "    x = current_test.shape[0]\n",
        "    y = current_test.shape[1]\n",
        "    z = current_test.shape[2]\n",
        "    score = np.zeros((x,y,z,2), dtype= 'float32')\n",
        "    score_num = np.zeros((x,y,z,2), dtype= 'int16')\n",
        "\n",
        "    count = 0\n",
        "    deplist = []\n",
        "    rowlist = []\n",
        "    collist = []\n",
        "    num = 0\n",
        "\n",
        "    box_test = np.zeros((batch,img_deps,img_rows,img_cols,1), dtype=\"float32\")\n",
        "    for deps in range(0,x-img_deps+window_deps,window_deps):\n",
        "        print (deps)\n",
        "        for rows in range(0, y-img_rows+window_rows, window_rows):\n",
        "            for cols in range(0,z-img_cols+window_cols,window_cols):\n",
        "                if deps>x-img_deps:\n",
        "                    deps = x-img_deps\n",
        "                elif rows > y-img_rows:\n",
        "                    rows = y-img_rows\n",
        "                elif cols>z-img_cols:\n",
        "                    cols = z-img_cols\n",
        "                elif deps>x-img_deps and rows > y - img_rows:\n",
        "                    deps = x - img_deps\n",
        "                    rows = y - img_rows\n",
        "                elif deps>x-img_deps and cols > z - img_cols:\n",
        "                    deps = x - img_deps\n",
        "                    cols = z - img_cols\n",
        "                elif rows>y-img_rows and cols > z-img_cols:\n",
        "                    rows = y - img_rows\n",
        "                    cols = z - img_cols\n",
        "                elif rows>y-img_rows and cols > z-img_cols and deps > x-img_deps:\n",
        "                    deps = x - img_deps\n",
        "                    rows = y - img_rows\n",
        "                    cols = z - img_cols\n",
        "                if count == batch:\n",
        "                    count = 0\n",
        "                    deplist = []\n",
        "                    rowlist = []\n",
        "                    collist = []\n",
        "                    box_test = np.zeros((batch, img_deps, img_rows, img_cols, 1), dtype=\"float32\")\n",
        "                patch_test = current_test[deps:deps+img_deps, rows:rows+img_rows, cols:cols+img_cols]\n",
        "                deplist.append(deps)\n",
        "                rowlist.append(rows)\n",
        "                collist.append(cols)\n",
        "                box_test[count,:,:,:,0] = patch_test\n",
        "                count += 1\n",
        "                del patch_test\n",
        "                if count == batch:\n",
        "                    num = num+1\n",
        "                    print ('num: ',num)\n",
        "                    print ('box:', box_test.shape)\n",
        "\n",
        "                    patch_test_mask = model.predict(box_test, verbose=0)\n",
        "\n",
        "                    if multiloss:\n",
        "                        patch_test_mask = patch_test_mask[2]\n",
        "                    patch_test_mask = K.softmax(patch_test_mask)\n",
        "                    patch_test_mask = K.eval(patch_test_mask)\n",
        "                    print ('predict finish')\n",
        "                    for i in range(batch):\n",
        "                        score[deplist[i]:deplist[i]+img_deps, rowlist[i]:rowlist[i]+img_rows, collist[i]:collist[i]+img_cols,:] += patch_test_mask[i]\n",
        "                        score_num[deplist[i]:deplist[i]+img_deps, rowlist[i]:rowlist[i]+img_rows, collist[i]:collist[i]+img_cols,:] += 1\n",
        "                    # print ('queue finish')\n",
        "                    del box_test, patch_test_mask, deplist, rowlist, collist\n",
        "    score = score / (score_num)\n",
        "    score2 = score[:,:,:,1]\n",
        "    return score2\n",
        "\n",
        "def get_binary_mask(score, id):\n",
        "    ## load affine\n",
        "    # label, header = load('/home/xmli/Data_gpu7/NewThresTestData/test-volume-' + str(id) + '.nii')\n",
        "    Segmask = GeneSeglivertumor(score)\n",
        "    Segmask = np.int16(Segmask)\n",
        "    return Segmask\n",
        "\n",
        "def GeneSeglivertumor(score):\n",
        "\n",
        "    score[score>=0.5] = 1\n",
        "    score[score<0.5] = 0\n",
        "    box = []\n",
        "    [liver_labels, num] = measure.label(score, return_num = True)\n",
        "    region = measure.regionprops(liver_labels)\n",
        "    for i in range(num):\n",
        "        box.append(region[i].area)\n",
        "    label_num = box.index(max(box))+1\n",
        "    liver_labels[liver_labels!=label_num] = 0\n",
        "    liver_labels[liver_labels==label_num] = 1\n",
        "\n",
        "    # labels = ndimage.binary_fill_holes(score).astype(int)\n",
        "    # labels = score\n",
        "    return liver_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-rR5zaZYECB"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, ZeroPadding2D, concatenate, Lambda, ZeroPadding3D, add\n",
        "from keras.layers.core import Dropout, Activation\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv3D, UpSampling3D, AveragePooling3D\n",
        "from keras.layers.pooling import AveragePooling2D, MaxPooling2D, MaxPooling3D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def conv_block3d(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv3D, 3x3 Conv3D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = '3dconv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = '3drelu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_x1_bn', momentum=1.0, trainable=False)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv3D(inter_channel, (1, 1, 1), name=conv_name_base+'_x1', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_x2_bn', momentum=1.0, trainable=False)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding3D((1, 1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv3D(nb_filter, (3, 3, 3), name=conv_name_base+'_x2', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "def dense_block3d(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block3d(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=4, name='3dconcat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "def transition_block3d(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = '3dconv' + str(stage) + '_blk'\n",
        "    relu_name_base = '3drelu' + str(stage) + '_blk'\n",
        "    pool_name_base = '3dpool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_bn', momentum=1.0)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv3D(int(nb_filter * compression), (1, 1, 1), name=conv_name_base, use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling3D((2, 2, 1), strides=(2, 2, 1), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "def DenseNet3D(img_input, nb_dense_block=4, growth_rate=32, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [3, 4, 12, 8]  # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding3D((3, 3, 3), name='3dconv1_zeropadding')(img_input)\n",
        "    x = Conv3D(nb_filter, (7, 7, 7), strides=(2, 2, 2), name='3dconv1', use_bias=False)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name='3dconv1_bn')(x)\n",
        "    x = Scale(axis=4, name='3dconv1_scale')(x)\n",
        "    x = Activation('relu', name='3drelu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding3D((1, 1, 1), name='3dpool1_zeropadding')(x)\n",
        "    x = MaxPooling3D((3, 3, 3), strides=(2, 2, 2), name='3dpool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx + 2\n",
        "        x, nb_filter = dense_block3d(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
        "                                   weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block3d(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate,\n",
        "                             weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block3d(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
        "                               weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name='3dconv' + str(final_stage) + '_blk_bn')(x)\n",
        "    x = Scale(axis=4, name='3dconv' + str(final_stage) + '_blk_scale')(x)\n",
        "    x = Activation('relu', name='3drelu' + str(final_stage) + '_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling3D(size=(2, 2, 1))(x)\n",
        "    conv_up0 = Conv3D(504, (3, 3, 3), padding=\"same\", name=\"3dconv_up0\")(up0)\n",
        "    bn_up0 = BatchNormalization(name=\"3dbn_up0\")(conv_up0)\n",
        "    ac_up0 = Activation('relu', name='3dac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling3D(size=(2, 2, 1))(ac_up0)\n",
        "    conv_up1 = Conv3D(224, (3, 3, 3), padding=\"same\", name=\"3dconv_up1\")(up1)\n",
        "    bn_up1 = BatchNormalization(name=\"3dbn_up1\")(conv_up1)\n",
        "    ac_up1 = Activation('relu', name='3dac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling3D(size=(2, 2, 1))(ac_up1)\n",
        "    conv_up2 = Conv3D(192, (3, 3, 3), padding=\"same\", name=\"3dconv_up2\")(up2)\n",
        "    bn_up2 = BatchNormalization(name=\"3dbn_up2\")(conv_up2)\n",
        "    ac_up2 = Activation('relu', name='3dac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling3D(size=(2, 2, 2))(ac_up2)\n",
        "    conv_up3 = Conv3D(96, (3, 3, 3), padding=\"same\", name=\"3dconv_up3\")(up3)\n",
        "    bn_up3 = BatchNormalization(name=\"3dbn_up3\")(conv_up3)\n",
        "    ac_up3 = Activation('relu', name='3dac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling3D(size=(2, 2, 2))(ac_up3)\n",
        "    conv_up4 = Conv3D(64, (3, 3, 3), padding=\"same\", name=\"3dconv_up4\")(up4)\n",
        "    bn_up4 = BatchNormalization(name=\"3dbn_up4\")(conv_up4)\n",
        "    ac_up4 = Activation('relu', name='3dac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv3D(3, (1, 1, 1), padding=\"same\", name='3dclassifer')(ac_up4)\n",
        "\n",
        "    return ac_up4, x\n",
        "\n",
        "\n",
        "\n",
        "def DenseUNet(img_input, nb_dense_block=4, growth_rate=48, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # Handle Dimension Ordering for different backends\n",
        "    global concat_axis\n",
        "    concat_axis = 3\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [6,12,36,24] # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False, trainable=True)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name='conv1_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name='conv1_scale')(x)\n",
        "    x = Activation('relu', name='relu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx+2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name='conv'+str(final_stage)+'_blk_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x)\n",
        "    x = Activation('relu', name='relu'+str(final_stage)+'_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling2D(size=(2,2))(x)\n",
        "    conv_up0 = Conv2D(768, (3, 3), padding=\"same\", name = \"conv_up0\", trainable=True)(up0)\n",
        "    bn_up0 = BatchNormalization(name = \"bn_up0\", momentum = 1, trainable=False)(conv_up0, training=False)\n",
        "    ac_up0 = Activation('relu', name='ac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling2D(size=(2,2))(ac_up0)\n",
        "    conv_up1 = Conv2D(384, (3, 3), padding=\"same\", name = \"conv_up1\", trainable=True)(up1)\n",
        "    bn_up1 = BatchNormalization(name = \"bn_up1\", momentum = 1, trainable=False)(conv_up1, training=False)\n",
        "    ac_up1 = Activation('relu', name='ac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling2D(size=(2,2))(ac_up1)\n",
        "    conv_up2 = Conv2D(96, (3, 3), padding=\"same\", name = \"conv_up2\", trainable=True)(up2)\n",
        "    bn_up2 = BatchNormalization(name = \"bn_up2\", momentum = 1, trainable=False)(conv_up2, training=False)\n",
        "    ac_up2 = Activation('relu', name='ac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2,2))(ac_up2)\n",
        "    conv_up3 = Conv2D(96, (3, 3), padding=\"same\", name = \"conv_up3\", trainable=True)(up3)\n",
        "    bn_up3 = BatchNormalization(name = \"bn_up3\", momentum = 1, trainable=False)(conv_up3, training=False)\n",
        "    ac_up3 = Activation('relu', name='ac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling2D(size=(2, 2))(ac_up3)\n",
        "    conv_up4 = Conv2D(64, (3, 3), padding=\"same\", name=\"conv_up4\", trainable=True)(up4)\n",
        "    bn_up4 = BatchNormalization(name=\"bn_up4\", momentum = 1, trainable=False)(conv_up4, training=False)\n",
        "    ac_up4 = Activation('relu', name='ac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv2D(3, (1,1), padding=\"same\", name='dense167classifer', trainable=True)(ac_up4)\n",
        "\n",
        "    return ac_up4, x\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_x1_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), name=conv_name_base+'_x1', use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_x2_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), name=conv_name_base+'_x2', use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
        "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
        "    pool_name_base = 'pool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv2D(int(nb_filter * compression), (1, 1), name=conv_name_base, use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "def slice(x, h1, h2):\n",
        "    \"\"\" Define a tensor slice function\n",
        "    \"\"\"\n",
        "    return x[:, :, :, h1:h2,:]\n",
        "def slice2d(x, h1, h2):\n",
        "\n",
        "    tmp = x[h1:h2,:,:,:]\n",
        "    tmp = tf.transpose(tmp, perm=[1, 2, 0, 3])\n",
        "    tmp = tf.expand_dims(tmp, 0)\n",
        "    return tmp\n",
        "\n",
        "def slice_last(x):\n",
        "\n",
        "    x = x[:,:,:,:,0]\n",
        "    return x\n",
        "def trans(x):\n",
        "\n",
        "    x = tf.transpose(x, perm=[0,3,1,2,4])\n",
        "    return x\n",
        "def trans_back(x):\n",
        "\n",
        "    x = tf.transpose(x, perm=[0,2,3,1,4])\n",
        "\n",
        "    return x\n",
        "def dense_rnn_net():\n",
        "\n",
        "    #  ************************3d volume input******************************************************************\n",
        "    img_input = Input(batch_shape=(1,512,512,8, 1), name='volumetric_data')\n",
        "\n",
        "    #  ************************(batch*d3cols)*2dvolume--2D DenseNet branch**************************************\n",
        "    input2d = Lambda(slice, arguments={'h1': 0, 'h2': 2})(img_input)\n",
        "    single = Lambda(slice, arguments={'h1':0, 'h2':1})(img_input)\n",
        "    input2d = concatenate([single, input2d], axis=3)\n",
        "    for i in range(8 - 2):\n",
        "        input2d_tmp = Lambda(slice, arguments={'h1': i, 'h2': i + 3})(img_input)\n",
        "        input2d = concatenate([input2d, input2d_tmp], axis=0)\n",
        "        if i == 8 - 3:\n",
        "            final1 = Lambda(slice, arguments={'h1': 8-2, 'h2': 8})(img_input)\n",
        "            final2 = Lambda(slice, arguments={'h1': 8-1, 'h2': 8})(img_input)\n",
        "            final = concatenate([final1, final2], axis=3)\n",
        "            input2d = concatenate([input2d, final], axis=0)\n",
        "    input2d = Lambda(slice_last)(input2d)\n",
        "\n",
        "    #  ******************************stack to 3D volumes *******************************************************\n",
        "    feature2d, classifer2d = DenseUNet(input2d, reduction=0.5)\n",
        "    res2d = Lambda(slice2d, arguments={'h1': 0, 'h2': 1})(classifer2d)\n",
        "    fea2d = Lambda(slice2d, arguments={'h1':0, 'h2':1})(feature2d)\n",
        "    for j in range(8 - 1):\n",
        "        score = Lambda(slice2d, arguments={'h1': j + 1, 'h2': j + 2})(classifer2d)\n",
        "        fea2d_slice = Lambda(slice2d, arguments={'h1': j + 1, 'h2': j + 2})(feature2d)\n",
        "        res2d = concatenate([res2d, score], axis=3)\n",
        "        fea2d = concatenate([fea2d, fea2d_slice], axis=3)\n",
        "\n",
        "    #  *************************** 3d DenseNet on 3D volume (concate with feature map )*********************************\n",
        "    res2d_input = Lambda(lambda x: x * 250)(res2d)\n",
        "    input3d_ori = Lambda(slice, arguments={'h1': 0, 'h2': 8})(img_input)\n",
        "    input3d = concatenate([input3d_ori, res2d_input], axis=4)\n",
        "    feature3d, classifer3d = DenseNet3D(input3d, reduction=0.5)\n",
        "\n",
        "    final = add([feature3d, fea2d])\n",
        "    final_conv = Conv3D(64, (3, 3, 3), padding=\"same\", name='fianl_conv')(final)\n",
        "    final_conv = Dropout(rate=0.3)(final_conv)\n",
        "    final_bn = BatchNormalization(name=\"final_bn\")(final_conv)\n",
        "    final_ac = Activation('relu', name='final_ac')(final_bn)\n",
        "    classifer = Conv3D(3, (1, 1, 1), padding=\"same\", name='2d3dclassifer')(final_ac)\n",
        "\n",
        "    model = Model( inputs = img_input,outputs = classifer, name='auto3d_residual_conv')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def dilated_resnet():\n",
        "    inputs = Input(batch_shape = (1, 512,512,8, 1))\n",
        "    conv1 = Conv3D(64, (3, 3, 3), padding = \"same\",kernel_initializer=\"normal\")(inputs)\n",
        "    bn0 = BatchNormalization()(conv1)\n",
        "    ac0 = Activation('relu')(bn0)\n",
        "    pool1 = MaxPooling3D(pool_size=(2, 2, 1))(ac0)\n",
        "\n",
        "    #  resudial block\n",
        "    conv2 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool1)\n",
        "    bn1 = BatchNormalization()(conv2)\n",
        "    ac1 = Activation('relu')(bn1)\n",
        "    conv3 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac1)\n",
        "    bn2 = BatchNormalization()(conv3)\n",
        "    pad1 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool1)\n",
        "    BN1 = BatchNormalization()(pad1)\n",
        "    sumb1 = add([BN1, bn2])\n",
        "    res1  = Activation('relu')(sumb1)\n",
        "\n",
        "    pool2 = MaxPooling3D(pool_size=(2, 2, 1))(res1)\n",
        "\n",
        "    #  resudial block\n",
        "    conv4 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool2)\n",
        "    bn3 = BatchNormalization()(conv4)\n",
        "    ac2 = Activation('relu')(bn3)\n",
        "    conv5 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac2)\n",
        "    bn4 = BatchNormalization()(conv5)\n",
        "    pad2 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool2)\n",
        "    BN2 = BatchNormalization()(pad2)\n",
        "    sumb2 = add([BN2, bn4])\n",
        "    res2  = Activation('relu')(sumb2)\n",
        "\n",
        "\n",
        "    pool3 = MaxPooling3D(pool_size=(2, 2, 1))(res2)\n",
        "\n",
        "    #  resudial block\n",
        "    conv6 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool3)\n",
        "    bn5 = BatchNormalization()(conv6)\n",
        "    ac3 = Activation('relu')(bn5)\n",
        "    conv7 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac3)\n",
        "    bn6 = BatchNormalization()(conv7)\n",
        "    pad3 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool3)\n",
        "    BN3 = BatchNormalization()(pad3)\n",
        "    sumb3 = add([BN3, bn6])\n",
        "    res3  = Activation('relu')(sumb3)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del1 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res3)\n",
        "    delbn1 = BatchNormalization()(del1)\n",
        "    delac1 = Activation('relu')(delbn1)\n",
        "    del2 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac1)\n",
        "    delbn2 = BatchNormalization()(del2)\n",
        "    deladd1 = add([res3, delbn2])\n",
        "    delres  = Activation('relu')(deladd1)\n",
        "\n",
        "    pool4 = MaxPooling3D(pool_size=(2, 2, 1))(delres)\n",
        "\n",
        "    #  resudial block\n",
        "    conv6_4 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool4)\n",
        "    bn5_4 = BatchNormalization()(conv6_4)\n",
        "    ac3_4 = Activation('relu')(bn5_4)\n",
        "    conv7_4 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac3_4)\n",
        "    bn6_4 = BatchNormalization()(conv7_4)\n",
        "    pad3_4 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool4)\n",
        "    BN3_4 = BatchNormalization()(pad3_4)\n",
        "    sumb3_4 = add([BN3_4, bn6_4])\n",
        "    res3_4  = Activation('relu')(sumb3_4)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del3 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res3_4)\n",
        "    delbn3 = BatchNormalization()(del3)\n",
        "    delac3 = Activation('relu')(delbn3)\n",
        "    del4 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac3)\n",
        "    delbn4 = BatchNormalization()(del4)\n",
        "    deladd2 = add([res3_4, delbn4])\n",
        "    delres2  = Activation('relu')(deladd2)\n",
        "\n",
        "\n",
        "    up0 = UpSampling3D(size=(2,2,1))(delres2)\n",
        "    pad4 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(delres)\n",
        "    BN4 = BatchNormalization()(pad4)\n",
        "    sumb4 = add([BN4, up0])\n",
        "\n",
        "    #  resudial block\n",
        "    conv8_1 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb4)\n",
        "    bn7_1 = BatchNormalization()(conv8_1)\n",
        "    ac4_1 = Activation('relu')(bn7_1)\n",
        "    conv9_1 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac4_1)\n",
        "    bn8_1 = BatchNormalization()(conv9_1)\n",
        "    pad5_1 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb4)\n",
        "    BN5_1 = BatchNormalization()(pad5_1)\n",
        "    sumb5_1 = add([BN5_1, bn8_1])\n",
        "    res4_1  = Activation('relu')(sumb5_1)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del5 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res4_1)\n",
        "    delbn5 = BatchNormalization()(del5)\n",
        "    delac5 = Activation('relu')(delbn5)\n",
        "    del6 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac5)\n",
        "    delbn6 = BatchNormalization()(del6)\n",
        "    deladd3 = add([res4_1, delbn6])\n",
        "    delres3  = Activation('relu')(deladd3)\n",
        "\n",
        "    up0_1 = UpSampling3D(size=(2,2,1))(delres3)\n",
        "    pad4_1 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res2)\n",
        "    BN4_1 = BatchNormalization()(pad4_1)\n",
        "    sumb4_1 = add([BN4_1, up0_1])\n",
        "\n",
        "    #  resudial block\n",
        "    conv8 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb4_1)\n",
        "    bn7 = BatchNormalization()(conv8)\n",
        "    ac4 = Activation('relu')(bn7)\n",
        "    conv9 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac4)\n",
        "    bn8 = BatchNormalization()(conv9)\n",
        "    pad5 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb4_1)\n",
        "    BN5 = BatchNormalization()(pad5)\n",
        "    sumb5 = add([BN5, bn8])\n",
        "    res4  = Activation('relu')(sumb5)\n",
        "\n",
        "    up1 = UpSampling3D(size=(2, 2, 1))(res4)\n",
        "    pad6 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res1)\n",
        "    BN6 = BatchNormalization()(pad6)\n",
        "    sumb6 = add([BN6, up1])\n",
        "\n",
        "    #  resudial block\n",
        "    conv10 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb6)\n",
        "    bn9 = BatchNormalization()(conv10)\n",
        "    ac5 = Activation('relu')(bn9)\n",
        "    conv11 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac5)\n",
        "    bn10 = BatchNormalization()(conv11)\n",
        "    pad7 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb6)\n",
        "    BN7 = BatchNormalization()(pad7)\n",
        "    sumb7 = add([BN7, bn10])\n",
        "    res5  = Activation('relu')(sumb7)\n",
        "\n",
        "    up2 = UpSampling3D(size=(2, 2, 1))(res5)\n",
        "    pad8 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(ac0)\n",
        "    BN8 = BatchNormalization()(pad8)\n",
        "    sumb8 = add([BN8, up2])\n",
        "\n",
        "    #  resudial block\n",
        "    conv12 = Conv3D(64, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb8)\n",
        "    bn11= BatchNormalization()(conv12)\n",
        "    ac6 = Activation('relu')(bn11)\n",
        "    conv13 = Conv3D(64, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac6)\n",
        "    bn12 = BatchNormalization()(conv13)\n",
        "    pad9 = Conv3D(64, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb8)\n",
        "    BN9 = BatchNormalization()(pad9)\n",
        "    sumb9 = add([BN9, bn12])\n",
        "    res6 = Activation('relu')(sumb9)\n",
        "\n",
        "    output3 = Conv3D(2, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res6)\n",
        "\n",
        "    # print (output3)\n",
        "\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[output3])\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYjlTRkmPTf-"
      },
      "source": [
        "\n",
        "#test.py\n",
        "\n",
        "from __future__ import print_function\n",
        "# import sys\n",
        "# sys.path.insert(0,'Keras-2.0.8')\n",
        "from keras import backend as K\n",
        "import os\n",
        "import numpy as np\n",
        "from medpy.io import load,save\n",
        "from keras.optimizers import SGD\n",
        "from scipy import ndimage\n",
        "from skimage import measure\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "K.set_image_dim_ordering('tf')  # Tensorflow dimension ordering in this code\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgveLqaWcnzu"
      },
      "source": [
        "img_test=img\n",
        "mask=mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U_cJ1qigLex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ec2b40-55f6-4831-acf7-6d250cc01a51"
      },
      "source": [
        "        id=0\n",
        "        print('-' * 30)\n",
        "        print('Loading model and preprocessing test data...' + str(id))\n",
        "        print('-' * 30)\n",
        "        model = dense_rnn_net()\n",
        "        model.load_weights('/content/gdrive/MyDrive/segmentation/model_best.hdf5')\n",
        "        sgd = SGD(lr=1e-2, momentum=0.9, nesterov=True)\n",
        "        model.compile(optimizer=sgd, loss=[weighted_crossentropy])\n",
        "\n",
        "        #  load data\n",
        "        # img_test, img_test_header = load('/content/data/myTrainingData/volume-'+str(id)+'.nii')\n",
        "        img_test -= 48\n",
        "\n",
        "        #  load liver mask\n",
        "        # mask, mask_header = load('/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/segmentation-'+str(id)+'.nii')\n",
        "        mask[mask==2]=1\n",
        "        mask = ndimage.binary_dilation(mask, iterations=1).astype(mask.dtype)\n",
        "        index = np.where(mask==1)\n",
        "        mini = np.min(index, axis = -1)\n",
        "        maxi = np.max(index, axis = -1)\n",
        "\n",
        "        print('-' * 30)\n",
        "        print('Predicting masks on test data...' + str(id))\n",
        "        print('-' * 30)\n",
        "        score1, score2 =  predict_tumor_inwindow(model, img_test, 3, mini, maxi)\n",
        "        K.clear_session()\n",
        "\n",
        "        result1 = score1\n",
        "        result2 = score2\n",
        "        result1[result1>=0.5]=1\n",
        "        result1[result1<0.5]=0\n",
        "        result2[result2>=0.9]=1\n",
        "        result2[result2<0.9]=0\n",
        "        result1[result2==1]=1\n",
        "\n",
        "        print('-' * 30)\n",
        "        print('Postprocessing on mask ...' + str(id))\n",
        "        print('-' * 30)\n",
        "\n",
        "        #  preserve the largest liver\n",
        "        Segmask = result2\n",
        "        box=[]\n",
        "        [liver_res, num] = measure.label(result1, return_num=True)\n",
        "        region = measure.regionprops(liver_res)\n",
        "        for i in range(num):\n",
        "            box.append(region[i].area)\n",
        "        label_num = box.index(max(box)) + 1\n",
        "        liver_res[liver_res != label_num] = 0\n",
        "        liver_res[liver_res == label_num] = 1\n",
        "\n",
        "        #  preserve the largest liver\n",
        "        mask = ndimage.binary_dilation(mask, iterations=1).astype(mask.dtype)\n",
        "        box = []\n",
        "        [liver_labels, num] = measure.label(mask, return_num=True)\n",
        "        region = measure.regionprops(liver_labels)\n",
        "        for i in range(num):\n",
        "            box.append(region[i].area)\n",
        "        label_num = box.index(max(box)) + 1\n",
        "        liver_labels[liver_labels != label_num] = 0\n",
        "        liver_labels[liver_labels == label_num] = 1\n",
        "        liver_labels = ndimage.binary_fill_holes(liver_labels).astype(int)\n",
        "\n",
        "\n",
        "        #  preserve tumor within ' largest liver' only\n",
        "        Segmask = Segmask * liver_labels\n",
        "        Segmask = ndimage.binary_fill_holes(Segmask).astype(int)\n",
        "        Segmask = np.array(Segmask,dtype='uint8')\n",
        "        liver_res = np.array(liver_res, dtype='uint8')\n",
        "        liver_res = ndimage.binary_fill_holes(liver_res).astype(int)\n",
        "        liver_res[Segmask == 1] = 2\n",
        "        liver_res = np.array(liver_res, dtype='uint8')\n",
        "        save(liver_res, 'results' + 'test-segmentation-' + str(id) + '.nii')\n",
        "\n",
        "        # del  Segmask, liver_labels, mask, region,label_num,liver_res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Loading model and preprocessing test data...0\n",
            "------------------------------\n",
            "------------------------------\n",
            "Predicting masks on test data...0\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 94%|| 48/51 [52:38<03:18, 66.13s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PpyfpmbckSt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}